---
title: "STAT 641 HW 1"
author: "Jack Cunningham (jgavc@tamu.edu)"
date: 02/06/2024
date-format: short
format:
  pdf:
    documentclass: scrartcl
    papersize: letter
editor: visual
engine: knitr
---

## Homework 2

1\)

We are tasked to show that the logistic function representation and logit representation for the logistic regression model are equivalent:

$$
p(X) = \frac{e^{\beta_0+\beta_1 X}}{1+e^{\beta_0+\beta_1 X}}
$$

Manipulating numerator and denominator on right side:

$$
p(X) = (1 + e^{\beta_0+\beta_1 X})p(X)\frac{1}{e^{\beta_0 + \beta_1 X}}p(X)
$$

$$
p(X) = p(X)(\frac{p(X)+e^{\beta_0 + \beta_1 X}p(X)}{e^{\beta_0 + \beta_1 X}})
$$

Dividing both sides by $p(X)$.

\$\$ 1 = \frac{p(X)+e^{\beta_0 + \beta_1 X}p(X)}{e^{\beta_0 + \beta_1 X}}

\$\$

$$
e^{\beta_0 + \beta_1 X} - e^{\beta_0 + \beta_1 X}p(X) = p(X)
$$

Factoring left side:

$$
e^{\beta_0 + \beta_1 X}(1 - p(X)) = p(X)
$$

$$
\frac{p(X)}{1-p(X)} = e^{\beta_0 + \beta_1 X}
$$

2\)

a\) We will use $1/10$ of the available observations since we are looking at observations within $10\%$ of our chosen X.

b\) We have $(X_1, X_2)$ uniformly distributed on $[0,1] \times[0,1]$. Since we want to look at observations within $10\%$ on both $X_1$ and $X_2$ we are left with a fraction of: $(\frac{1}{10})(\frac{1}{10}) = \frac{1}{100}$ .

c\) Extending our previous answer, with $p = 100$ we will only use $(\frac{1}{10})^{100}$ of the available observations.

d\) As p increases the fraction of observations within a given range is extremely small this leads us to either extend our range to find "like" observations or be left with very few observations that drive our prediction.

e\) If we want to use, on average, $10\%$ of our training observations to inform a prediction on a test observation we can treat the space utilized as a hyper-cube. The hyper-cube then has a volume of 0.1.

The function for the volume of a hyper-cube is $(x)^n$ where $x$ is the length of each equal side and n is the number of sides.

Our objective is to solve for the length of each side, so $x = V^{1/n}$ where V is the volume.

For $p=1$ we have a length of $x = (.1)^{1}=.1$.

For $p=2$ we have a length of $x = (.1)^{1/2}=.3162278$

For $p=100$ we have a length of $x = (.1)^{1/100}=.9772372$

As p increases in order to get a certain amount of nearby values we need extend the range of acceptable values in each predictor. We can see that at high p, such as 100, the range for each predictor is .9772. This causes concern as the nearby values aren't nearby at all.

3\)

We are given a training data set and apply two classification procedures:

1.  Logistic regression with an error rate of 20% on training data and 30% on test data.

2.  1-nearest neighbors with an average error rate of 18% on training and test data.

With the information we are given the test error is the best metric for assessing model performance. In order to compare the performance of the two methods we need to obtain the test error rate of 1-nearest neighbors.

The nature of 1-nearest neighbors is assigning an observation to its closest observation. After the model is trained and we ask it to classify an already seen observation it will guess the correct class every time. Therefore the training error for 1-nearest neighbors will be 0%. Since we are given the average error rate of 18% we can easily find the test error rate, 36%.

We would choose logistic regression in this case as it has a lower error rate than 1-nearest neighbors.
