---
title: "STAT 639 HW 3"
author: "Jack Cunningham (jgavc@tamu.edu)"
date: 04/02/2024
date-format: short
format:
  pdf:
    documentclass: scrartcl
    papersize: letter
editor: visual
engine: knitr
---

1\)

a\) K-fold cross-validation is implemented by splitting the observed data into k non-overlapping sets with close to equal size, these resulting sets are called folds. For each step in the procedure a fold is kept out as a validation set. A chosen model is fit using the k-1 remaining folds and evaluated using the validation set. The evaluation criteria depends on the setting and objective. For instance a regression problem may utilize mean-squared error, while a classififcation problem may use the error rate. This procedure is done k times, with each of the k folds is used as a validation set once. The evaluation metric is then averaged and returned.

b\)

i\) The k-fold cross-validation approach has notable advantages over the validation set approach.

Firstly, the validation set approach struggles with the variability inherent to the selection of the held out set. For example consider 20% of the data is held out. There are instances where the selected 20% are on the higher end, where they are on the lower end, where outliers are selected and so on. If the same procedure was ran again results could be wildly different depending on the randomly selected validation set. The k-fold cross-validation approach on the other hand uses each K split as a validation set and averages the evaluation criteria. This reduces the variance of the approach in its estimate of test error.

Secondly, the validation set approach often overestimates the test error we would see if the entire data set was used to train. Continuing the previous example, only 80% of our observed data was utilized in training our model and 20% was held out. There is a balance between having enough data to adequately train the model and enough to adequately evaluate it. The K-fold cross-validation approach allows us to utilize all of the observed data to train the model and thus does not tend to overestimate the test error.

The main disadvantage the k-fold cross-validation approach has is its increased computational requirement. With the validation set approach a model needs to be fit only one time, the K-fold cross validation approach requires it be fit k times. Depending on the computational complexity of a model this can be restrictive. But with a reasonable number k chosen, with complexity in mind, this disadvantage can be adjusted for. Additionally for many models the computational tax being paid is not too high.

c\)

i\) The LOOCV approach is a special version of the K-fold cross validation approach where k = n.

The first strength of the LOOCV approach is its sensibility and thus its lack of bias in estimating test error. The LOOCV approach uses everything but one observation each time a model is fit and evaluates its performance on that one observation. This is repeated n times and the evaluation criteria is averaged out. It is similar to the actual process of seeing a new observation and attempting to estimate it, we are using all the other information possible to make our prediction. Therefore the estimate of test error is approximately unbiased.

The benefit of being unbiased can become a curse in many scenarios. As previously discussed the trade-off between bias and variance is paramount for evaluating our prediction. The variance of the LOOCV approach is high due to the n fitted models all being very similar, therefore correlated, to each other. The average of highly correlated quantities has high variance compared to moderately correlated quantities. The k-fold-cross-validation approach strikes more of a middle ground between bias and variance. In many cases this is an advantage in its ability to estimate test error.

A main drawback of the LOOCV approach is its computational intensity. As previously discussed the LOOCV approach fits a model n times. This makes it unrealistic for situations where a model is computationally taxing or n is large. K-fold cross-validation has a clear advantage in this area, the difference between fitting a model n times and 5 or 10 times (commonly selected values of k) is a world of difference computationally.

2\)

```{r}
#| include: false
library(ISLR2)
library(boot)
```

a\)

```{r}
lm.fit <- glm(default ~ income + balance, data = Default, family = binomial)
summary(lm.fit)$coefficients
```

After fitting a logistic model we see that the standard deviation of the income and balance estimate is 4.9852e-06 and 2.2737e-04 respectively.

b\)

```{r}
boot.fn <- function(data, index){
  coef(glm(default ~ income + balance, data = Default, family = binomial,
           subset = index))
}
```

c\)

```{r}
#| cache = TRUE
set.seed(5)
boot(Default, boot.fn, 1000)
```

d\)

The standard errors obtained from the glm function and bootstrap function are very close to each other.

3\)

a\) iii. is correct. Least squares has zero bias if linear assumptions are met. The lasso introduces bias and is also a linear model, so it is less flexible. If the squared bias is less than the decrease in variance we would see improved predictions.

b\) iii. is correct. Least squares has zero bias if linear assumptions are met. Ridge regression introduces bias and is also a linear model, so it is less flexible. If the squared bias is less than the decrease in variance we would see improved predictions.

c\) ii. is correct. A non-linear method makes less assumptions about the form of the true function than a linear method, therefore it is more flexible. We would expect prediction accuracy to improve if the bias we reduced is more than the increase in variance.

4\)

a\) iv. Steadily decrease is correct. When s = 0 we have a null model with coefficients equal to zero. That is when bias is at its highest and variance is 0. As s increases the flexibility of the model is increased. Training RSS will decrease as flexibility increases, we are following the training data closer and heading towards the least squares estimate (which solely focuses on minimizes RSS).

b\) ii. Decrease initially and then start increasing in a U shape is correct. . When s = 0 we have a null model with coefficients equal to zero. That is when bias is at its highest and variance is 0. As s increases the flexibility of the model is increased. Test RSS, similar to other test errors, will be minimized somewhere in the middle ground on the bias and variance trade-off. After that point the increased variance will outpace the reduction in bias and test RSS will increase.

c\) iii. Steadily increase is correct. When s = 0 we have a null model with coefficients equal to zero. That is when bias is at its highest and variance is 0. As s increases the flexibility of the model is increased, naturally variance is also increased.

d\) iv. Steadily decrease is correct. When s = 0 we have a null model with coefficients equal to zero. That is when bias is at its highest and variance is 0. As s increases the flexibility of the model is increased, naturally bias is decreasing.

e\) v. Remain constant is correct. Irreducible error cannot, as the name suggests, be reduced through increasing the flexibility of a model.

5\)

a\)

Splitting data and creating necessary objects for further steps.

```{r}
library(ISLR2)
library(glmnet)

x <- model.matrix(Apps ~ ., College)[, -1]
y <- College$Apps

set.seed(11)
train <- sample(1:nrow(x), nrow(x) / 2)
test <- (-train)
y.test <- y[test]

College_train <- College[train, ]
College_test <- College[test, ]
```

b\)

```{r}
apps_lse <- lm(Apps ~ ., College[train, ])
apps_lse_pred <- predict(apps_lse, College[test, ])
mean((apps_lse_pred - y.test)^2)
```

The test MSE we end up with is 1026096.

c\)

```{r}
cv.out_ridge <- cv.glmnet(x[train, ], y[train], alpha  = 0)
plot(cv.out_ridge)
```

```{r}
bestlam_ridge <- cv.out_ridge$lambda.min
bestlam_ridge
```

```{r}
apps_
```
